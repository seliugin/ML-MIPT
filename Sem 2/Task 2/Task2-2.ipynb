{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 2. Классификация даты документа.\n",
        "### Михаил Селюгин\n",
        "\n",
        "Устанавливаем и импортируем необходимые библиотеки"
      ],
      "metadata": {
        "id": "GJkMVpTAYYiF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xxy7UjcR_eH"
      },
      "outputs": [],
      "source": [
        "!pip install bigartm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oki7ZlsqOT4X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import artm\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from google.colab import drive\n",
        "from bisect import bisect_left\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "pBManatPn2rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Копируем к себе предоставленные данные по модели\n",
        "\n",
        "import shutil\n",
        "shutil.copytree(src='/content/drive/MyDrive/Colab Notebooks/topicmodeling', dst='/content/topicmodel')"
      ],
      "metadata": {
        "id": "GO2jhhgyvCeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBMZg3rXoQ-D"
      },
      "source": [
        "## Предобработка данных и установка предобученной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLxAhmxgqSpl"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('topicmodel/lenta-ru-proccess.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RixxyaB39N-"
      },
      "outputs": [],
      "source": [
        "data['year'] = data.apply(lambda row: int(row['date'].split('-')[2]), axis=1)\n",
        "\n",
        "all_years = sorted(data['year'].unique())\n",
        "print(all_years)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gFwwMQtixJs"
      },
      "outputs": [],
      "source": [
        "file_train = 'topicmodel/vw_data/texts_train.vw.txt'\n",
        "file_test = 'topicmodel/vw_data/texts_test.vw.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia8a575Ey9M5"
      },
      "outputs": [],
      "source": [
        "train_mask = np.random.default_rng(1).random(len(data)) < 0.9\n",
        "split_regexp = re.compile(r'\\W+')\n",
        "\n",
        "year_train = []\n",
        "year_test = []\n",
        "\n",
        "with open(file_train, 'w') as ftrain, open(file_test, 'w') as ftest:\n",
        "    for i in tqdm(range(len(data))):\n",
        "        text = data.loc[i]['text']\n",
        "        text = (split_regexp.sub(' ', text).strip()).split()\n",
        "        if len(text) < 100:\n",
        "            continue\n",
        "        text = ' '.join(text)\n",
        "\n",
        "        year = data.loc[i]['year']\n",
        "        if train_mask[i]:\n",
        "            ftrain.write(f'doc_{len(year_train)} {text}\\n')\n",
        "            year_train.append(year)\n",
        "        else:\n",
        "            ftest.write(f'doc_{len(year_test)} {text}\\n')\n",
        "            year_test.append(year)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "jJz5XyTJuzhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Батчи либо формируем, либо берем уже сформированные"
      ],
      "metadata": {
        "id": "hpaq2PAzv8IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  bv_train = artm.BatchVectorizer(data_path='topicmodel/batches_train',\n",
        "#                                         data_format='batches')\n",
        "#  bv_test = artm.BatchVectorizer(data_path='topicmodel/batches_test',\n",
        "#                                         data_format='batches')"
      ],
      "metadata": {
        "id": "S3EHBN4ovTqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bv_train = artm.BatchVectorizer(data_path=file_train, data_format='vowpal_wabbit', batch_size=10000, \n",
        "    target_folder='topicmodel/batches_train')\n",
        "\n",
        "bv_test = artm.BatchVectorizer(data_path=file_test, data_format='vowpal_wabbit', batch_size=10000, \n",
        "    target_folder='topicmodel/batches_test')"
      ],
      "metadata": {
        "id": "JEkL9UPmbga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = artm.Dictionary()\n",
        "dictionary.gather(data_path='topicmodeling/batches_train')"
      ],
      "metadata": {
        "id": "d8guAdD8luBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Инициализация моделей без регуляризации и с ней (выгружаем предобученные)"
      ],
      "metadata": {
        "id": "KYqW5oI1vNpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_without = artm.load_artm_model('./drive/MyDrive/Colab Notebooks/topicmodeling/model/without_regular/')\n",
        "\n",
        "model_without.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary), overwrite=True)"
      ],
      "metadata": {
        "id": "NjegAYMElJjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_last = artm.load_artm_model('./drive/MyDrive/Colab Notebooks/topicmodeling/model/last_model/')\n",
        "\n",
        "model_last.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary), overwrite=True)"
      ],
      "metadata": {
        "id": "ToB7m5MMlSKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сравним скоры моделей по perplexity"
      ],
      "metadata": {
        "id": "UwGGey57mQ8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.plot(model_without.score_tracker['perplexity'].value, label='model1_perplexity')\n",
        "plt.plot(model_last.score_tracker['perplexity'].value, label='model2_perplexity')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CxCMHS8ukNy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Векторизация по темам"
      ],
      "metadata": {
        "id": "_jzE51abqsIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(artm: artm.ARTM, bv: artm.BatchVectorizer, years: list):\n",
        "    values = np.array(artm.transform(bv).values).T\n",
        "    dataset = []\n",
        "    for i in range(len(values)):\n",
        "        year_onehot = np.zeros(len(all_years), dtype='float32')\n",
        "        year_onehot[bisect_left(all_years, years[i])] = 1.0\n",
        "        dataset.append((values[i], year_onehot))\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "nAlrYg5etLt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('topicmodel/vw_data/y_valid.vw.txt') as f:\n",
        "#     y_valid = f.read().split('\\n')\n",
        "\n",
        "# with open('topicmodel/vw_data/y_train.vw.txt') as f:\n",
        "#     y_train = f.read().split('\\n')"
      ],
      "metadata": {
        "id": "K8pndLXNwv7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EGMFFShUbf2"
      },
      "outputs": [],
      "source": [
        "train_dataset1 = make_dataset(model_without, bv_train, year_train)\n",
        "test_dataset1 = make_dataset(model_without, bv_test, year_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2 = make_dataset(model_last, bv_train, year_train)\n",
        "test_dataset2 = make_dataset(model_last, bv_test, year_test)"
      ],
      "metadata": {
        "id": "g_AyFXX3DPzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_pd = model_without.transform(bv_train)\n",
        "\n",
        "# X_train = []\n",
        "# for i in tqdm(range(len(y_train))):\n",
        "#     X_train.append(X_train_pd[i].values)\n",
        "\n",
        "# X_train = np.array(X_train)\n",
        "# y_train = np.array(y_train)\n",
        "\n",
        "# from sklearn.utils import shuffle\n",
        "# X_train, y_train = shuffle(X_train, y_train)\n",
        "# X_train = X_train[:20000]\n",
        "# y_train = y_train[:20000]"
      ],
      "metadata": {
        "id": "JeDMaF7UxPF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c03Ji95XU2bt"
      },
      "outputs": [],
      "source": [
        "print(\"Train data length:\", len(train_dataset1), len(train_dataset1))\n",
        "print(\"Test data length:\", len(test_dataset1), len(test_dataset1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuNN4PvbubWO"
      },
      "source": [
        "## Инициализация классификатора (многослойной нейронной сети)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JOmukCtpngu"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_size, \n",
        "        output_size, \n",
        "        hidden_size, \n",
        "        n_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(input_size, hidden_size))\n",
        "        modules.append(nn.BatchNorm1d(hidden_size))\n",
        "        modules.append(nn.ReLU())\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            modules.append(nn.Linear(hidden_size, hidden_size))\n",
        "            modules.append(nn.BatchNorm1d(hidden_size))\n",
        "            modules.append(nn.ReLU())\n",
        "\n",
        "        modules.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "        self.layers = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        for m in self.layers:\n",
        "            x = m(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TugYwiboudEk"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train_epoch(self, train_loader, epoch, writer):\n",
        "        self.model.train()\n",
        "        loop = tqdm(train_loader)\n",
        "        for step, (x, y) in enumerate(loop):\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.criterion(self.model(x), y)\n",
        "            curr_step = epoch * len(loop) + step\n",
        "            writer.add_scalar('loss/train', loss, curr_step)\n",
        "            loop.set_description(f'{epoch}. train_loss: {loss.item():.3f}')\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, val_loader, epoch, writer):\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        loop = tqdm(val_loader)\n",
        "        for x, y in loop:\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            loss = self.criterion(self.model(x), y).item()\n",
        "            total_loss += loss\n",
        "            loop.set_description(f'{epoch}. Val_loss: {loss:.3f}')\n",
        "        total_loss /= len(val_loader)\n",
        "        writer.add_scalar('loss/valid', loss, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeQvkhYo-zj_"
      },
      "source": [
        "## Обучение и тестирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SKa2DdE-xPk"
      },
      "outputs": [],
      "source": [
        "def train(train_dataset,\n",
        "          test_dataset, \n",
        "          n_epochs=10,\n",
        "          hidden_size=256, \n",
        "          n_layers=5,\n",
        "          writer=None):\n",
        "    model = Model(input_size=len(train_dataset[0][0]), output_size=len(all_years),\n",
        "                          hidden_size=hidden_size, n_layers=n_layers)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "    dl_train = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    dl_valid = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "    trainer = Trainer(model, optimizer)\n",
        "    for epoch in range(n_epochs):\n",
        "        trainer.train_epoch(dl_train, epoch, writer)\n",
        "        trainer.validate(dl_valid, epoch, writer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8GF9iZw-1Du"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(log_dir='runs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3GaCYm4DT1v"
      },
      "source": [
        "### Зависимость качества предсказания от модели ARTM (с регуляризацией или без)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lROqwmv4DXtQ"
      },
      "outputs": [],
      "source": [
        "train(train_dataset=train_dataset1, test_dataset=test_dataset1, writer=writer)\n",
        "train(train_dataset=train_dataset2, test_dataset=test_dataset2, writer=writer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pktBle8Dgau"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"runs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQTEuoNZqk7v"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "!zip -r task2.zip ./runs/\n",
        "files.download('task2.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы\n",
        "\n",
        "Судя по большим значениям лосса, классификация получилась не слишком успешная, как с регуляризацией, так и без нее.\n",
        "\n",
        "С чем это связано? Вряд ли с классификатором, скорее с искомой artm моделью и особенностями ее работы."
      ],
      "metadata": {
        "id": "l7wMR5R2ya9r"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "task2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}